{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from  torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from torchvision.models.resnet import BasicBlock, Bottleneck\n",
    "\n",
    "import utils\n",
    "import vision_transformer as vits\n",
    "from vision_transformer import DINOHead, VisionTransformer\n",
    "\n",
    "from my_utils import myResNet, ReturnEmbWrapper\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\")\n",
    "from dataloaders import load\n",
    "\n",
    "from importlib import import_module, reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(name=\"MNIST\", normal_class=1, seed=0):\n",
    "    if name == \"MNIST\": stats = ((0.1307,), (0.3081,))\n",
    "    elif name == \"CIFAR10\": stats = ((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "    elif name == \"FashionMNIST\": stats = ((0.2860,), (0.3530,))\n",
    "    elif name == \"SVHN\": stats = ((0.4377, 0.4438, 0.4728), (0.198, 0.201, 0.197))\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(*stats),\n",
    "    ])\n",
    "    # if name == \"MNIST\":\n",
    "    #     test_dataset = datasets.MNIST(root=\"/workspace/angular_dino/datasets\", download=False, train=False, transform=val_transform)\n",
    "    # elif name == \"CIFAR10\":\n",
    "    #     test_dataset = datasets.CIFAR10(root=\"/workspace/angular_dino/datasets/CIFAR10\", download=False, train=False, transform=val_transform)\n",
    "    # elif name == \"FashionMNIST\":\n",
    "    #     test_dataset = datasets.FashionMNIST(root=\"/workspace/angular_dino/datasets\", download=False, train=False, transform=val_transform)\n",
    "    # elif name == \"SVHN\":\n",
    "    #     test_dataset = datasets.SVHN(root=\"/workspace/angular_dino/datasets/SVHN\", download=False, split=\"test\", transform=val_transform)\n",
    "    test_dataset = load(name, normal_class=[normal_class], unseen_anomaly=[0],return_test_subset=True, seed=seed, return_id = True)\n",
    "    pu_dataset, pu_val_dataset = load(\n",
    "        name=name,\n",
    "        batch_size=False,\n",
    "        normal_class=[normal_class],\n",
    "        unseen_anomaly=[0],\n",
    "        labeled_anomaly_class=False, \n",
    "        n_train = 4500,\n",
    "        n_valid = 500,\n",
    "        # n_test = 2000,\n",
    "        n_unlabeled_normal = 4500, #n_unlabeled_normal\n",
    "        n_unlabeled_anomaly = 250, #n_unlabeled_anomaly\n",
    "        n_labeled_anomaly = 250, #n_labeled_anomaly\n",
    "        return_extra_test_loader= False,\n",
    "        return_subset=True,\n",
    "        return_unl_pos_subset=False, \n",
    "        transform = val_transform,\n",
    "        seed=seed, \n",
    "        return_id = False\n",
    "        )\n",
    "    pu_loader = DataLoader(dataset=pu_dataset, batch_size=100, shuffle=False, num_workers=8,\n",
    "           drop_last=False, pin_memory=True)\n",
    "    pu_val_loader = DataLoader(dataset=pu_val_dataset, batch_size=100, shuffle=False, num_workers=8,\n",
    "           drop_last=False, pin_memory=True)\n",
    "    return pu_dataset, pu_val_dataset, pu_loader, pu_val_loader, test_dataset\n",
    "\n",
    "def get_result(model, cp_path, dataset=\"MNIST\", prototype_vec_weight=None, normal_class=1, seed=0, varbose=True):\n",
    "    # print(\"loading check point\")\n",
    "    # load_pretrained_backbone_head(model, cp_path, checkpoint_key=\"Teacher\")\n",
    "    print(\"preparing dataset\")\n",
    "    pu_dataset, pu_val_dataset, pu_loader, pu_val_loader, test_dataset = get_datasets(dataset, normal_class, seed=seed)\n",
    "    print(\"obtaining prototype vecs\")\n",
    "    pu_unlabel_vec_dict, pu_positive_vec_dict, pu_normal_prototype_dict, pu_normal_prototype_dict_id = get_prototype_vec_ids(model, pu_loader, pu_val_loader, prototype_vec_weight)\n",
    "    if varbose:\n",
    "        print(f\"unlabel_prototype_vec_ids = {list(pu_unlabel_vec_dict.keys())[:5]}, total dot = {list(map(lambda x: round(x, 4), pu_unlabel_vec_dict.values()))[:5]}\")\n",
    "        print(f\"positive_prototype_vec_ids = {list(pu_positive_vec_dict.keys())[:5]}, total dot = {list(map(lambda x: round(x, 4), pu_positive_vec_dict.values()))[:5]}\")\n",
    "        print(f\"noraml_prototype_vec_ids = {list(pu_normal_prototype_dict.keys())[:5]}, total dot = {list(map(lambda x: round(x, 4), pu_normal_prototype_dict.values()))[:5]}\")\n",
    "        print(f\"noraml_prototype_vec_ids = {pu_normal_prototype_dict_id}\")\n",
    "\n",
    "    if varbose: print(\"calculating score\")\n",
    "    y_true, y_score, y_each_class, dot_each_class, dot_pos = calc_y_score(model, test_dataset, pu_normal_prototype_dict_id, pu_unlabel_vec_dict, pu_positive_vec_dict, pu_normal_prototype_dict, prototype_vec_weight, normal_class, varbose)\n",
    "    if varbose: \n",
    "        print(\"y_true, y_score example \")\n",
    "        for n, (t, s) in enumerate(zip(y_true, y_score)):\n",
    "            print(t, s)\n",
    "            if n == 9: break\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    auroc = roc_auc_score(y_true, y_score)\n",
    "    if varbose:\n",
    "        print(\"showing ROC curve\")\n",
    "        plt.figure(figsize=(16,9))\n",
    "        plt.plot(fpr, tpr, marker='o')\n",
    "        plt.xlabel('FPR: False positive rate')\n",
    "        plt.ylabel('TPR: True positive rate')\n",
    "        plt.grid()\n",
    "        plt.title(f\"AUROC = {auroc:.4f}\")\n",
    "    print(f\"AUROC: {auroc}\")\n",
    "    \n",
    "    if varbose:\n",
    "        conf_score = np.copy(y_score)\n",
    "        conf_score = np.where(conf_score>0.5, 1, conf_score).astype(int)\n",
    "        print(\"confusion_matrix, thresholds=0.5\")\n",
    "        print(confusion_matrix(y_true, conf_score.tolist()))\n",
    "        plot_class_scores(y_each_class, normal_class)\n",
    "        plot_class_prototype_dot(dot_each_class, dot_pos)\n",
    "    return auroc\n",
    "\n",
    "def get_prototype_vec(cp_path):\n",
    "    prototype_vec_weight = nn.utils.weight_norm(nn.Linear(64, 100, bias=False)).cuda()\n",
    "    prototype_vec_weight.weight_g.data.fill_(1)\n",
    "    prototype_vec_weight.weight_g.requires_grad = False\n",
    "    state_prototype_vec = torch.load(cp_path, map_location=\"cpu\")[\"t_prototype_vec\"]\n",
    "    prototype_vec_weight.load_state_dict(state_prototype_vec)\n",
    "    prototype_vec_weight.eval()\n",
    "    prototype_vec_weight(torch.eye(64).cuda(non_blocking=True))\n",
    "    return prototype_vec_weight\n",
    "\n",
    "def construct_model(dataset_name=\"MNIST\"):\n",
    "    embed_dim=64\n",
    "    arcface_family_conf = {\"name\":False}\n",
    "    if \"MNIST\" in dataset_name: ch=1\n",
    "    else: ch=3\n",
    "\n",
    "    teacher = myResNet(BasicBlock, [2,2,2,2], normalize=True)\n",
    "    # teacher = myResNet(Bottleneck, [3,4,6,3], normalize=True)\n",
    "    teacher.conv1 = nn.Conv2d(ch, 64, kernel_size=7, stride=1, padding=3,bias=False)\n",
    "    teacher.fc = nn.Linear(teacher.fc.in_features, out_features=embed_dim, bias=True)\n",
    "    # teacher = CustomWrapper(\n",
    "    #         teacher,\n",
    "    #         DINOHead(embed_dim, 512, False, arcface_family_conf=arcface_family_conf,\n",
    "    #             nlayers=3,\n",
    "    #             hidden_dim=512,\n",
    "    #             bottleneck_dim=128,\n",
    "    #             ),\n",
    "    #     )\n",
    "    teacher = ReturnEmbWrapper(\n",
    "            teacher,\n",
    "            DINOHead(\n",
    "                64,\n",
    "                256,\n",
    "                use_bn=False,\n",
    "                norm_last_layer=False,\n",
    "                nlayers=2, #3,\n",
    "                hidden_dim=128, #512\n",
    "                bottleneck_dim=64 #128,\n",
    "                ),\n",
    "        )\n",
    "    \n",
    "    return teacher\n",
    "\n",
    "\n",
    "def load_pretrained_backbone_head(model, cp_path, checkpoint_key=\"Teacher\"):\n",
    "    model.cuda()\n",
    "    state_dict = torch.load(cp_path, map_location=\"cpu\")[\"teacher\"]\n",
    "    state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "    backbone_state = {k.replace(\"backbone.\", \"\"):v for k,v in state_dict.items() if k.startswith(\"backbone\")}\n",
    "    head_state = {k.replace(\"head.\", \"\"):v for k,v in state_dict.items() if k.startswith(\"head\")}\n",
    "    model.backbone.load_state_dict(backbone_state, strict=False)\n",
    "    model.head.load_state_dict(head_state, strict=False)\n",
    "    model.eval()\n",
    "\n",
    "def plot_embedding(val_loader, model, prototype_vec_weight):\n",
    "    model.eval()\n",
    "    emb_list=defaultdict(list)\n",
    "\n",
    "    for images, ids in val_loader:\n",
    "        with torch.no_grad():\n",
    "            output = model.backbone(images.cuda(non_blocking=True))\n",
    "            for n, out in enumerate(output):\n",
    "                emb_list[ids[n].item()].append(out.cpu())\n",
    "            \n",
    "    emb_list = dict(sorted(emb_list.items(), key=lambda x: x[0]))\n",
    "    emb_numpy = torch.vstack(sum(list(emb_list.values()), [])).numpy()\n",
    "    prototype_vec_numpy = prototype_vec_weight.weight.data.cpu().detach().numpy()[:]\n",
    "\n",
    "    emb_prototype_numpy = np.vstack([emb_numpy, prototype_vec_numpy])\n",
    "    # tsne = TSNE(n_components=2, random_state = 0, perplexity = 30, n_iter = 1000)\n",
    "    _umap = umap.UMAP(n_neighbors=15, n_components=2, metric=\"cosine\", min_dist=0.7, spread=0.7)\n",
    "    # emb_tsne = tsne.fit_transform(emb_prototype_numpy)\n",
    "    emb_tsne = _umap.fit_transform(emb_prototype_numpy)\n",
    "    return emb_tsne, emb_list\n",
    "\n",
    "def select_unique_random_indices(arr, num=1):\n",
    "    # ユニークな要素を取得\n",
    "    unique_elements = set(arr)\n",
    "    unique_indices = defaultdict(list)\n",
    "    for id, cls in enumerate(arr):\n",
    "        unique_indices[cls].append(id)\n",
    "    selected_indices = {k:random.sample(list(v), min(num, len(v))) for k,v in unique_indices.items()}\n",
    "\n",
    "    # # ユニークな要素のインデックスを保持する辞書\n",
    "    # unique_indices = defaultdict(list)\n",
    "    # for index, value in enumerate(arr):\n",
    "    #     unique_indices[value].append(index)\n",
    "    \n",
    "    # # print(unique_indices)\n",
    "    # unique_indices = dict(sorted(unique_indices.items(), key=lambda x:x[0]))\n",
    "    # # print(unique_indices)\n",
    "    # # ユニークな要素からランダムにインデックスを選択\n",
    "    # # selected_indices = [unique_indices[element] for element in random.sample(list(unique_elements), len(unique_elements))]\n",
    "    # if num==1:\n",
    "    #     selected_indices = {k:random.choice(list(v)) for k,v in unique_indices.items()}\n",
    "    # else:\n",
    "    #     selected_indices = {k:random.sample(list(v), num) for k,v in unique_indices.items()}\n",
    "    return selected_indices\n",
    "\n",
    "def plot_class_scores(y_each_class, normal_class=1):\n",
    "    plt.figure(figsize=(12,9))\n",
    "    for n, (k, v) in enumerate(y_each_class.items()):\n",
    "        plt.subplot(2,5, n+1)\n",
    "        plt.bar(np.arange(len(v))[:], v[:], label=f\"class id{k}\\nmean:{np.mean(v):.4f}\")\n",
    "        plt.ylim(-0.1,0.5)\n",
    "        plt.legend(handlelength=0, frameon=False)\n",
    "    plt.suptitle(\"normal prototype vec cos similarity\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.figure(figsize=(12,9))\n",
    "    anomaly_scores = []\n",
    "    for k,v in y_each_class.items():\n",
    "        if k!=normal_class:\n",
    "            anomaly_scores.extend(v)\n",
    "        else:\n",
    "            normal_scores = v\n",
    "\n",
    "    max_ = max(max(anomaly_scores), max(normal_scores))\n",
    "    min_ = min(min(anomaly_scores), min(normal_scores))\n",
    "    anomaly_scores = (np.array(anomaly_scores)-min_) / (max_-min_)\n",
    "    normal_scores = (np.array(normal_scores)-min_) / (max_-min_)\n",
    "\n",
    "    # bn = np.linspace(-1, 1, 10)\n",
    "    bn = np.array(range(0,11))/10\n",
    "    # bn = np.linspace(min_, max_, 10)\n",
    "    plt.hist(anomaly_scores, label=\"normlity score from anomarly samples\", bins=bn, color=\"red\", ec=\"black\", alpha=0.7)\n",
    "    plt.hist(normal_scores, label=\"normlity score from normal samples\", bins=bn, color=\"blue\", ec=\"black\", alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.title(\"normality score hist ano vs norm\")\n",
    "    print(f\"anomaly scores: {[round(i, 4)for i in anomaly_scores]}\")\n",
    "    print(f\"normal scores: {[round(i, 4)for i in normal_scores]}\")\n",
    "\n",
    "def plot_class_prototype_dot(dot_each_class, dot_pos):\n",
    "    plt.figure(figsize=(12,9))\n",
    "    for n, (k, v) in enumerate(dot_each_class.items()):\n",
    "        max_id = np.argsort(v)[:5]\n",
    "        plt.subplot(3,5, n+1)\n",
    "        plt.bar(np.arange(len(v))[:], v[:], label=f\"class id{k}\\nmax_ids:{max_id}\")\n",
    "        plt.ylim(-0.1,0.5)\n",
    "        plt.legend(handlelength=0, frameon=False)\n",
    "    plt.subplot(3,5, n+2)\n",
    "    max_id = np.argsort(dot_pos)[:5]\n",
    "    plt.bar(np.arange(len(dot_pos))[:], dot_pos[:], label=f\"positive\\nmax_ids:{max_id}\")\n",
    "    plt.ylim(-0.1,0.5)\n",
    "    plt.legend(handlelength=0, frameon=False)\n",
    "\n",
    "def calc_y_score(model, val_dataset, pu_normal_prototype_dict_id, pu_unlabel_vec_dict, pu_positive_vec_dict, pu_normal_prototype_dict, prototype_vec_weight=None, normal_class=1, varbose=True):\n",
    "    # normal_prototype_vec = head_state[\"last_layer.weight_v\"][pu_normal_prototype_dict_id,:]\n",
    "    if not prototype_vec_weight:\n",
    "        normal_prototype_vec = model.head.last_layer.weight_v.detach().cpu().clone()\n",
    "        normal_prototype_vec /= torch.norm(normal_prototype_vec, p=2)\n",
    "    else:\n",
    "        normal_prototype_vec = prototype_vec_weight.weight.data.detach().cpu().clone()\n",
    "\n",
    "    # if not prototype_vec_weight:\n",
    "    #     normal_prototype_vec = model.head.last_layer.weight_v.detach().cpu().clone()[pu_normal_prototype_dict_id,:]\n",
    "    #     normal_prototype_vec /= torch.norm(normal_prototype_vec, p=2)\n",
    "    # else:\n",
    "    #     normal_prototype_vec = prototype_vec_weight.weight.data.detach().cpu().clone()[pu_normal_prototype_dict_id,:]\n",
    "    #     # normal_prototype_vec = prototype_vec_weight.weight.data.detach().cpu().clone()\n",
    "    # print(f\"代表ベクトルのshape: {normal_prototype_vec.shape}, norm: {normal_prototype_vec.norm(dim=1, p=2)}\")\n",
    "\n",
    "    ################\n",
    "    top5_normal_prototype_ids = list(pu_normal_prototype_dict.keys())[:10]\n",
    "    # top5_normal_prototype_ids = [2,93,57,51,6]\n",
    "    # top5_normal_prototype_ids = list(pu_unlabel_vec_dict.keys())[:5]\n",
    "    top5_positive_prototype_ids = list(pu_positive_vec_dict.keys())[:10]\n",
    "    # top5_normal_prototype_ids = [i for i in top5_normal_prototype_ids if i not in top5_positive_prototype_ids]\n",
    "\n",
    "    # top5_normal_prototype_vecs= head_state[\"last_layer.weight_v\"][top5_normal_prototype_ids,:]\n",
    "    if not prototype_vec_weight:\n",
    "        top5_normal_prototype_vecs= normal_prototype_vec[top5_normal_prototype_ids,:]\n",
    "    else:\n",
    "        top5_normal_prototype_vecs = normal_prototype_vec[top5_normal_prototype_ids,:]\n",
    "        top5_positive_prototype_vecs = normal_prototype_vec[top5_positive_prototype_ids,:]\n",
    "    if varbose:\n",
    "        print(f\"top5正常代表ベクトルのindex: {top5_normal_prototype_ids}\")\n",
    "        print(f\"top5異常代表ベクトルのindex: {top5_positive_prototype_ids}\")\n",
    "    # print(f\"top5正常代表ベクトルのshape: {top5_normal_prototype_vecs.shape}, norm: {top5_normal_prototype_vecs.norm(dim=1, p=2)}\")\n",
    "    top5_normal_prototype_wights = list(pu_normal_prototype_dict.values())[:10]\n",
    "    top5_positive_prototype_wights = list(pu_positive_vec_dict.values())[:10]\n",
    "    if varbose:\n",
    "        print(f\"top5_normal_prototype_weights: {top5_normal_prototype_wights}\")\n",
    "    top5_normal_prototype_wights = nn.functional.softmax(torch.tensor(top5_normal_prototype_wights)/0.5, dim=0, dtype=torch.float32)\n",
    "    top5_positive_prototype_wights = nn.functional.softmax(torch.tensor(top5_positive_prototype_wights)/0.5, dim=0, dtype=torch.float32)\n",
    "    if varbose:\n",
    "        print(f\"top5_normal_prototype_softmax_weights: {top5_normal_prototype_wights}\")\n",
    "        print(f\"top5_positive_prototype_softmax_weights: {top5_positive_prototype_wights}\")\n",
    "    ################\n",
    "\n",
    "    random.seed(42)\n",
    "    #正常データのindex \n",
    "    # try: indices = val_dataset.targets\n",
    "    # except: indices = val_dataset.labels\n",
    "    try: targets = val_dataset.dataset.targets\n",
    "    except: targets = val_dataset.dataset.labels\n",
    "    if not torch.is_tensor(targets): targets = torch.tensor(targets)\n",
    "    indices = targets[val_dataset.indices]\n",
    "    normal_indices = (indices==normal_class).nonzero().flatten()\n",
    "    anomaly_indices = (indices!=normal_class).nonzero().flatten()\n",
    "    print(len(normal_indices), len(anomaly_indices))\n",
    "    \n",
    "    # if not torch.is_tensor(indices):\n",
    "    #     indices = torch.tensor(indices)\n",
    "    # normal_indices = (indices==1).nonzero().flatten()\n",
    "    # normal_indices = random.sample(normal_indices.tolist(), 1000)\n",
    "    # #異常データのindex\n",
    "    # anomaly_indices = (indices!=1).nonzero().flatten()\n",
    "    # anomaly_indices = random.sample(anomaly_indices.tolist(), 1000)\n",
    "\n",
    "    #結果の配列\n",
    "    y_true = []\n",
    "    y_score = []\n",
    "    y_each_class = defaultdict(list)\n",
    "    dot_each_class = defaultdict(lambda: np.zeros(normal_prototype_vec.shape[0]))\n",
    "    class_sample_num = {k:0 for k in range(10)}\n",
    "    dot_pos = np.zeros(normal_prototype_vec.shape[0])\n",
    "\n",
    "    for sample, id in val_dataset:\n",
    "        # if i in normal_indices:\n",
    "        #     y_true.append(0)\n",
    "        # elif i in anomaly_indices:\n",
    "        #     y_true.append(1)\n",
    "        # else: continue\n",
    "        # sample, id = val_dataset.__getitem__(i)\n",
    "        # print(id)\n",
    "        # if id==0: \n",
    "        #     print(id)\n",
    "        #     continue\n",
    "        if id==normal_class: y_true.append(0)\n",
    "        elif id!=normal_class: y_true.append(1)\n",
    "        if varbose:\n",
    "            print(id)\n",
    "            print(Counter(y_true))\n",
    "        sample = sample.unsqueeze(0)\n",
    "        if not prototype_vec_weight:\n",
    "            with torch.no_grad():\n",
    "                head_emb = model(sample.cuda(non_blocking=True), return_emb=True)[0].cpu()\n",
    "                head_emb = nn.functional.normalize(head_emb.unsqueeze(0), p=2)[0]\n",
    "                score = (normal_prototype_vec @ head_emb).item()\n",
    "                y_score.append(score)\n",
    "                y_each_class[id].append(score)\n",
    "                \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                backbone_emb = model.backbone(sample.cuda(non_blocking=True))[0].cpu()\n",
    "                # backbone_emb = nn.functional.normalize(backbone_emb.unsqueeze(0), p=2)[0]\n",
    "                # y_score.append(((normal_prototype_vec @ backbone_emb).item()))\n",
    "\n",
    "                # score = (top5_normal_prototype_vecs @ backbone_emb).mean().item() #meanではなくmaxも試す\n",
    "                # score = (top5_normal_prototype_vecs @ backbone_emb).max().item() #meanではなくmaxも試す\n",
    "                # score = ((top5_normal_prototype_vecs @ backbone_emb) @ top5_normal_prototype_wights).item()\n",
    "                # score = ((top5_normal_prototype_vecs @ backbone_emb) * top5_normal_prototype_wights).max().item()\n",
    "                score = (top5_normal_prototype_vecs @ backbone_emb).mean().item() - \\\n",
    "                    (top5_positive_prototype_vecs @ backbone_emb).mean().item()\n",
    "                # score = ((top5_normal_prototype_vecs @ backbone_emb) @ top5_normal_prototype_wights).item() - \\\n",
    "                #     ((top5_positive_prototype_vecs @ backbone_emb) @ top5_positive_prototype_wights).item()*0.5\n",
    "                # score = ((top5_normal_prototype_vecs @ backbone_emb) - (top5_positive_prototype_vecs @ backbone_emb)*0.5).max().item()\n",
    "                # score = (((top5_normal_prototype_vecs @ backbone_emb) * top5_normal_prototype_wights) - \\\n",
    "                #     ((top5_positive_prototype_vecs @ backbone_emb) * top5_positive_prototype_wights)*0.5).max().item()\n",
    "                y_score.append(score)\n",
    "                if varbose:\n",
    "                    print((1-score)/2)\n",
    "                # print((2-score)/4)\n",
    "                # y_score.append( ((top5_normal_prototype_vecs @ backbone_emb) @ top5_normal_prototype_wights).item())\n",
    "\n",
    "                # backbone_emb = model.backbone(sample.cuda(non_blocking=True))[0]\n",
    "                # sf_out = nn.functional.softmax(prototype_vec_weight(backbone_emb)).cpu()\n",
    "                # y_score.append(sf_out[pu_normal_prototype_dict_id])\n",
    "                y_each_class[id].append(score)\n",
    "                dot_each_class[id] += (normal_prototype_vec @ backbone_emb).numpy()\n",
    "                class_sample_num[id] += 1\n",
    "                if id != normal_class:\n",
    "                    dot_pos += (normal_prototype_vec @ backbone_emb).numpy()\n",
    "\n",
    "\n",
    "        ##################\n",
    "        # y_score.append((top5_normal_prototype_vecs @ head_emb).mean().item())\n",
    "        # y_score.append( ((top5_normal_prototype_vecs @ head_emb) @ top5_normal_prototype_wights).item())\n",
    "        ##################\n",
    "    # print(normal_prototype_vec.shape, backbone_emb.shape, normal_prototype_vec.norm(dim=1), backbone_emb.norm(), (normal_prototype_vec @ backbone_emb).numpy())\n",
    "    y_score = np.array(y_score)\n",
    "    y_score = (y_score - y_score.min()) / (y_score.max() - y_score.min())\n",
    "    y_score = 1 - y_score\n",
    "    # y_score = (1-y_score)/2\n",
    "    # y_score = (2-y_score)/4\n",
    "    y_each_class = dict(sorted(y_each_class.items(), key=lambda x: x[0]))\n",
    "    dot_each_class = dict(sorted(dot_each_class.items(), key=lambda x: x[0]))\n",
    "    dot_each_class = {k:v/class_sample_num[k] for k,v in dot_each_class.items()}\n",
    "    if varbose:\n",
    "        plt.figure(figsize=(16,9))\n",
    "        plt.bar(list(range(len(y_score))), y_score)\n",
    "        # plt.bar(list(range(len(y_score))), y_true)\n",
    "        print(Counter(y_true))\n",
    "    return y_true, y_score, y_each_class, dot_each_class, dot_pos/1000\n",
    "\n",
    "def get_prototype_vec_ids(model, pu_loader, pu_valloader, prototype_vec_weight=None):\n",
    "    # import pdb; pdb.set_trace()\n",
    "\n",
    "    num_unlabel = 0\n",
    "    num_positive = 0\n",
    "    for _,i in pu_loader:\n",
    "        i = i.tolist()\n",
    "        c = Counter(i)\n",
    "        num_unlabel+=c[0]\n",
    "        num_positive+=c[1]\n",
    "    for _,i in pu_valloader:\n",
    "        i = i.tolist()\n",
    "        c = Counter(i)\n",
    "        num_unlabel+=c[0]\n",
    "        num_positive+=c[1]\n",
    "    print(\"num unl pos\",num_unlabel, num_positive)\n",
    "\n",
    "    num_unlabel = 0\n",
    "    num_positive = 0\n",
    "    pu_num = 0\n",
    "    val_num=0\n",
    "    for _,i in pu_loader:\n",
    "        for k in i:\n",
    "            if k==0: num_unlabel+=1\n",
    "            elif k==1: num_positive+=1\n",
    "            else: raise NotImplementedError()\n",
    "            pu_num+=1\n",
    "    for _,i in pu_valloader:\n",
    "        for k in i:\n",
    "            if k==0: num_unlabel+=1\n",
    "            elif k==1: num_positive+=1\n",
    "            else: raise NotImplementedError()\n",
    "            val_num+=1\n",
    "    print(\"num unl pos\",num_unlabel, num_positive, pu_num, val_num)\n",
    "\n",
    "    pu_unlabel_vec_dict = defaultdict(float)\n",
    "    pu_positive_vec_dict = defaultdict(float)\n",
    "\n",
    "    #####\n",
    "    for images, unl_pos in pu_loader:\n",
    "        images = images.cuda(non_blocking=True)\n",
    "        if not prototype_vec_weight:\n",
    "            with torch.no_grad():\n",
    "                output = model(images)[0][0]\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                output = prototype_vec_weight(model.backbone(images))\n",
    "        output =  output.cpu().detach()\n",
    "        sorted_out, sorted_arg = torch.sort(output, dim=1, descending=True)\n",
    "        sorted_out, sorted_arg = sorted_out.numpy()[:, :5], sorted_arg.numpy()[:, :5]\n",
    "        for n, (out, arg) in enumerate(zip(sorted_out, sorted_arg)):\n",
    "            if unl_pos[n] == 0:\n",
    "                for o, a in zip(out, arg):\n",
    "                    pu_unlabel_vec_dict[a] += o/num_unlabel\n",
    "            elif unl_pos[n] == 1:\n",
    "                for o, a in zip(out, arg):\n",
    "                    pu_positive_vec_dict[a] += o/num_positive\n",
    "            else: raise NotImplementedError\n",
    "    \n",
    "    for images, unl_pos in pu_valloader:\n",
    "        images = images.cuda(non_blocking=True)\n",
    "        if not prototype_vec_weight:\n",
    "            with torch.no_grad():\n",
    "                output = model(images)[0][0]\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                output = prototype_vec_weight(model.backbone(images))\n",
    "        output =  output.cpu().detach()\n",
    "        sorted_out, sorted_arg = torch.sort(output, dim=1, descending=True)\n",
    "        sorted_out, sorted_arg = sorted_out.numpy()[:, :5], sorted_arg.numpy()[:, :5]\n",
    "        for n, (out, arg) in enumerate(zip(sorted_out, sorted_arg)):\n",
    "            if unl_pos[n] == 0:\n",
    "                for o, a in zip(out, arg):\n",
    "                    pu_unlabel_vec_dict[a] += o/num_unlabel\n",
    "            elif unl_pos[n] == 1:\n",
    "                for o, a in zip(out, arg):\n",
    "                    pu_positive_vec_dict[a] += o/num_positive\n",
    "            else: raise NotImplementedError\n",
    "\n",
    "    pu_unlabel_vec_dict = dict(sorted(pu_unlabel_vec_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "    pu_positive_vec_dict = dict(sorted(pu_positive_vec_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "    pu_normal_prototype_dict = pu_unlabel_vec_dict.copy()\n",
    "\n",
    "    for k in pu_positive_vec_dict:\n",
    "        if k in pu_normal_prototype_dict.keys():\n",
    "            pu_normal_prototype_dict[k] -= pu_positive_vec_dict[k]\n",
    "\n",
    "    pu_normal_prototype_dict = dict(sorted(pu_normal_prototype_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "    pu_normal_prototype_dict_id = list(pu_normal_prototype_dict.keys())[0]\n",
    "    return  pu_unlabel_vec_dict, pu_positive_vec_dict, pu_normal_prototype_dict, pu_normal_prototype_dict_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"test/fashionmnist/trials/tea-stu-arcface-m0.5-step50/trial1/fmnist_class1_resnet18_ptnum100_ep200_bs128_lambda1.0-warmup100_tea-stu-arcface-m0.5-step50-num10_useweight_smallhead_emb64_lr0.0005-weighted-sampler_test\"\n",
    "\n",
    "cp_path = \"/workspace/angular_dino/dino/outputs/\"+ task_name +\"/checkpoints/checkpoint.pth\"\n",
    "dataset_name = \"FashionMNIST\" #\"CIFAR10\" #SVHN #FashionMNIST #MNIST\n",
    "\n",
    "teacher = construct_model(dataset_name)\n",
    "load_pretrained_backbone_head(teacher, cp_path, checkpoint_key=\"Teacher\")\n",
    "prototype_vec_weight = get_prototype_vec(cp_path)\n",
    "auroc = get_result(teacher, cp_path, dataset_name, prototype_vec_weight, normal_class=1, seed=0, varbose=True)\n",
    "\n",
    "# score = 0.0\n",
    "# seeds = list(range(0,31)) #list(range(31,61)) #\n",
    "# score_dict={}\n",
    "# for s in seeds:\n",
    "#     scores = []\n",
    "#     for i in range(1,10):\n",
    "#         task_name= f\"test/mnist/trials/tea-stu-arcface-m0.5-step50/trial5/mnist_class{i}_resnet18_ptnum100_ep200_bs128_lambda1.0-warmup100_tea-stu-arcface-m0.5-step50-num10_useweight_smallhead_emb64_lr0.0005-weighted-sampler_test\"\n",
    "#         cp_path = \"/workspace/angular_dino/dino/outputs/\"+ task_name +\"/checkpoints/checkpoint.pth\"\n",
    "\n",
    "#         teacher = construct_model(dataset_name)\n",
    "#         load_pretrained_backbone_head(teacher, cp_path, checkpoint_key=\"Teacher\")\n",
    "#         prototype_vec_weight = get_prototype_vec(cp_path)\n",
    "#         auroc = get_result(teacher, cp_path, dataset_name, prototype_vec_weight, normal_class=i, seed=s, varbose=False)\n",
    "\n",
    "#         scores.append(auroc)\n",
    "#     new_score = np.mean(scores)\n",
    "#     print(f\"seed:{s} auroc:{new_score:.4f}\")\n",
    "#     score_dict[str(s)] = new_score\n",
    "#     if new_score > score:\n",
    "#         score=new_score\n",
    "#     if score > 0.9966: break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
