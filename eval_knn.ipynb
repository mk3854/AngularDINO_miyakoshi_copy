{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.distributed as dist\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as pth_transforms\n",
    "from torchvision import models as torchvision_models\n",
    "\n",
    "import utils\n",
    "import vision_transformer as vits\n",
    "from vision_transformer import DINOHead\n",
    "from my_utils import ResNetMNIST, plot_embed, DINOLoss, train_one_epoch, load_pretrained_backbone_head, CustomWrapper\n",
    "\n",
    "from torch.utils.data.dataset import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 1,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 9,\n",
       " 2,\n",
       " 5,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 9,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 9,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 7,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 7,\n",
       " 9,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 9,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 3,\n",
       " 9,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 9,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 4,\n",
       " 9,\n",
       " 0,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 5,\n",
       " 9,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 6,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 9,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 9,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 7,\n",
       " 6,\n",
       " 9,\n",
       " 1,\n",
       " 6,\n",
       " 7,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 9,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 8,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 7,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 9,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 9,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 0,\n",
       " 7,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 9,\n",
       " 7,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 9,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 7,\n",
       " 0,\n",
       " 9,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 9,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 4,\n",
       " 8,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 9,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 9,\n",
       " 5,\n",
       " 8,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 9,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 9,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 2,\n",
       " 7,\n",
       " 6,\n",
       " 9,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " ...]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s[-1] for s in dataset_train.__getitems__(list(range(len(dataset_train))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded with 7000 train and 3000 val imgs.\n",
      "Model ResNet Built\n",
      "Take key teacher in provided checkpoint dict\n",
      "Pretrained weights found at /workspace/puae/dino/outputs/emb10_arcface_m0.1_warmup0/checkpoints/checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n",
      "Extracting features for train set...\n",
      "Storing features into tensor of shape torch.Size([7000, 10])\n",
      "  [ 0/70]  eta: 0:00:25    time: 0.362045  data: 0.341849  max mem: 269\n",
      "  [10/70]  eta: 0:00:02    time: 0.036043  data: 0.031195  max mem: 269\n",
      "  [20/70]  eta: 0:00:01    time: 0.003599  data: 0.000117  max mem: 269\n",
      "  [30/70]  eta: 0:00:00    time: 0.004344  data: 0.000160  max mem: 269\n",
      "  [40/70]  eta: 0:00:00    time: 0.004087  data: 0.000167  max mem: 269\n",
      "  [50/70]  eta: 0:00:00    time: 0.004062  data: 0.000118  max mem: 269\n",
      "  [60/70]  eta: 0:00:00    time: 0.004873  data: 0.000110  max mem: 269\n",
      "  [69/70]  eta: 0:00:00    time: 0.004818  data: 0.000082  max mem: 269\n",
      " Total time: 0:00:00 (0.010079 s / it)\n",
      "Extracting features for val set...\n",
      "Storing features into tensor of shape torch.Size([3000, 10])\n",
      "  [ 0/30]  eta: 0:00:09    time: 0.302218  data: 0.292722  max mem: 269\n",
      "  [10/30]  eta: 0:00:00    time: 0.032203  data: 0.026800  max mem: 269\n",
      "  [20/30]  eta: 0:00:00    time: 0.003900  data: 0.000136  max mem: 269\n",
      "  [29/30]  eta: 0:00:00    time: 0.002840  data: 0.000062  max mem: 269\n",
      " Total time: 0:00:00 (0.015181 s / it)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import Subset\n",
    "# args.port=str(29500)\n",
    "# utils.init_distributed_mode(args)\n",
    "# print(\"git:\\n  {}\\n\".format(utils.get_sha()))\n",
    "# print(\"\\n\".join(\"%s: %s\" % (k, str(v)) for k, v in sorted(dict(vars(args)).items())))\n",
    "# cudnn.benchmark = True\n",
    "\n",
    "transform = pth_transforms.Compose([\n",
    "        pth_transforms.ToTensor(),\n",
    "    ])\n",
    "class ReturnIndexDataset():\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset=dataset\n",
    "    def __getitem__(self, idx):\n",
    "        img, lab = self.dataset.__getitem__(idx)\n",
    "        return img, idx\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "dataset = datasets.MNIST(root=\"../datasets\", download=False, train=False, transform=transform)\n",
    "dataset_train, dataset_val = torch.utils.data.random_split(dataset, [0.7, 0.3])\n",
    "index_datset_train = ReturnIndexDataset(dataset_train)\n",
    "index_datset_val = ReturnIndexDataset(dataset_val)\n",
    "\n",
    "# dataset_train = ReturnIndexDataset(os.path.join(args.data_path, \"train\"), transform=transform)\n",
    "# dataset_val = ReturnIndexDataset(os.path.join(args.data_path, \"val\"), transform=transform)\n",
    "sampler = torch.utils.data.DistributedSampler(index_datset_train, shuffle=False)\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    index_datset_train,\n",
    "    sampler=sampler,\n",
    "    batch_size=args.batch_size_per_gpu,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    index_datset_val,\n",
    "    batch_size=args.batch_size_per_gpu,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "print(f\"Data loaded with {len(index_datset_train)} train and {len(index_datset_val)} val imgs.\")\n",
    "args.emb_dim = 10\n",
    "\n",
    "# ============ building network ... ============\n",
    "if \"resnet\" in args.arch.lower():\n",
    "    embed_dim = args.emb_dim\n",
    "    model = ResNetMNIST(num_blocks=[2, 2, 2, 2], num_classes=embed_dim)\n",
    "    print(f\"Model ResNet Built\")\n",
    "elif \"vit\" in args.arch:\n",
    "    model = vits.__dict__[args.arch](patch_size=args.patch_size, num_classes=0)\n",
    "    print(f\"Model {args.arch} {args.patch_size}x{args.patch_size} built.\")\n",
    "elif \"xcit\" in args.arch:\n",
    "    model = torch.hub.load('facebookresearch/xcit:main', args.arch, num_classes=0)\n",
    "elif args.arch in torchvision_models.__dict__.keys():\n",
    "    model = torchvision_models.__dict__[args.arch](num_classes=0)\n",
    "    model.fc = nn.Identity()\n",
    "else:\n",
    "    print(f\"Architecture {args.arch} non supported\")\n",
    "    sys.exit(1)\n",
    "model.cuda()\n",
    "utils.load_pretrained_weights(model, args.pretrained_weights, args.checkpoint_key, args.arch, args.patch_size)\n",
    "model.eval()\n",
    "\n",
    "# ============ extract features ... ============\n",
    "print(\"Extracting features for train set...\")\n",
    "train_features = extract_features(model, data_loader_train, args.use_cuda)\n",
    "print(\"Extracting features for val set...\")\n",
    "test_features = extract_features(model, data_loader_val, args.use_cuda)\n",
    "\n",
    "if utils.get_rank() == 0:\n",
    "    train_features = nn.functional.normalize(train_features, dim=1, p=2)\n",
    "    test_features = nn.functional.normalize(test_features, dim=1, p=2)\n",
    "\n",
    "# train_labels = torch.tensor([s[-1] for s in dataset_train.samples]).long()\n",
    "# test_labels = torch.tensor([s[-1] for s in dataset_val.samples]).long()\n",
    "train_labels = torch.tensor([s[-1] for s in dataset_train.__getitems__(list(range(len(dataset_train))))]).long()\n",
    "test_labels = torch.tensor([s[-1] for s in dataset_val.__getitems__(list(range(len(dataset_val))))]).long()\n",
    "\n",
    "# save features and labels\n",
    "if args.dump_features and dist.get_rank() == 0:\n",
    "    torch.save(train_features.cpu(), os.path.join(args.dump_features, \"trainfeat.pth\"))\n",
    "    torch.save(test_features.cpu(), os.path.join(args.dump_features, \"testfeat.pth\"))\n",
    "    torch.save(train_labels.cpu(), os.path.join(args.dump_features, \"trainlabels.pth\"))\n",
    "    torch.save(test_labels.cpu(), os.path.join(args.dump_features, \"testlabels.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_pipeline(args):\n",
    "    # ============ preparing data ... ============\n",
    "    transform = pth_transforms.Compose([\n",
    "        # pth_transforms.Resize(256, interpolation=3),\n",
    "        # pth_transforms.CenterCrop(224),\n",
    "        pth_transforms.ToTensor(),\n",
    "        pth_transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.MNIST(root=\"../datasets\", download=False, train=False, transform=transform)\n",
    "    dataset_train, dataset_val = torch.utils.data.random_split(dataset, [0.7, 0.3])\n",
    "    index_datset_train = ReturnIndexDataset(dataset_train)\n",
    "    index_datset_val = ReturnIndexDataset(dataset_val)\n",
    "\n",
    "    # dataset_train = ReturnIndexDataset(os.path.join(args.data_path, \"train\"), transform=transform)\n",
    "    # dataset_val = ReturnIndexDataset(os.path.join(args.data_path, \"val\"), transform=transform)\n",
    "    sampler = torch.utils.data.DistributedSampler(index_datset_train, shuffle=False)\n",
    "    data_loader_train = torch.utils.data.DataLoader(\n",
    "        index_datset_train,\n",
    "        sampler=sampler,\n",
    "        batch_size=args.batch_size_per_gpu,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        index_datset_val,\n",
    "        batch_size=args.batch_size_per_gpu,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    print(f\"Data loaded with {len(index_datset_train)} train and {len(index_datset_val)} val imgs.\")\n",
    "\n",
    "    # ============ building network ... ============\n",
    "    if \"resnet\" in args.arch.lower():\n",
    "        embed_dim = args.emb_dim\n",
    "        if \"head\" in args.arch.lower():\n",
    "            teacher = ResNetMNIST(num_blocks=[2, 2, 2, 2], num_classes=embed_dim)\n",
    "            model = CustomWrapper(\n",
    "            teacher,\n",
    "            DINOHead(embed_dim, 2000, False, arcface_family_conf={\"name\":False}),\n",
    "        )\n",
    "        else:\n",
    "            model = ResNetMNIST(num_blocks=[2, 2, 2, 2], num_classes=embed_dim)\n",
    "        print(f\"Model ResNet emb_dim:{args.emb_dim} Built\")\n",
    "    elif \"vit\" in args.arch:\n",
    "        model = vits.__dict__[args.arch](patch_size=args.patch_size, num_classes=0)\n",
    "        print(f\"Model {args.arch} {args.patch_size}x{args.patch_size} built.\")\n",
    "    elif \"xcit\" in args.arch:\n",
    "        model = torch.hub.load('facebookresearch/xcit:main', args.arch, num_classes=0)\n",
    "    elif args.arch in torchvision_models.__dict__.keys():\n",
    "        model = torchvision_models.__dict__[args.arch](num_classes=0)\n",
    "        model.fc = nn.Identity()\n",
    "    else:\n",
    "        print(f\"Architecture {args.arch} non supported\")\n",
    "        sys.exit(1)\n",
    "    model.cuda()\n",
    "    if \"resnet\" in args.arch.lower() and \"head\" in args.arch.lower():\n",
    "        load_pretrained_backbone_head(model, args.pretrained_weights, \"teacher\")\n",
    "        print(f\"load resnet backbone and head. Then, delete last layer\")\n",
    "    else:\n",
    "        utils.load_pretrained_weights(model, args.pretrained_weights, args.checkpoint_key, args.arch, args.patch_size)\n",
    "    model.eval()\n",
    "\n",
    "    # ============ extract features ... ============\n",
    "    print(\"Extracting features for train set...\")\n",
    "    train_features = extract_features(model, data_loader_train, args.use_cuda)\n",
    "    print(\"Extracting features for val set...\")\n",
    "    test_features = extract_features(model, data_loader_val, args.use_cuda)\n",
    "\n",
    "    if utils.get_rank() == 0:\n",
    "        train_features = nn.functional.normalize(train_features, dim=1, p=2)\n",
    "        test_features = nn.functional.normalize(test_features, dim=1, p=2)\n",
    "\n",
    "    # train_labels = torch.tensor([s[-1] for s in dataset_train.samples]).long()\n",
    "    # test_labels = torch.tensor([s[-1] for s in dataset_val.samples]).long()\n",
    "    train_labels = torch.tensor([s[-1] for s in dataset_train.__getitems__(list(range(len(dataset_train))))]).long()\n",
    "    test_labels = torch.tensor([s[-1] for s in dataset_val.__getitems__(list(range(len(dataset_val))))]).long()\n",
    "\n",
    "    # save features and labels\n",
    "    if args.dump_features and dist.get_rank() == 0:\n",
    "        torch.save(train_features.cpu(), os.path.join(args.dump_features, \"trainfeat.pth\"))\n",
    "        torch.save(test_features.cpu(), os.path.join(args.dump_features, \"testfeat.pth\"))\n",
    "        torch.save(train_labels.cpu(), os.path.join(args.dump_features, \"trainlabels.pth\"))\n",
    "        torch.save(test_labels.cpu(), os.path.join(args.dump_features, \"testlabels.pth\"))\n",
    "    return train_features, test_features, train_labels, test_labels\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(model, data_loader, use_cuda=True, multiscale=False):\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    features = None\n",
    "    for samples, index in metric_logger.log_every(data_loader, 10):\n",
    "        samples = samples.cuda(non_blocking=True)\n",
    "        index = index.cuda(non_blocking=True)\n",
    "        if multiscale:\n",
    "            feats = utils.multi_scale(samples, model)\n",
    "        else:\n",
    "            feats = model(samples).clone()\n",
    "\n",
    "        # init storage feature matrix\n",
    "        if dist.get_rank() == 0 and features is None:\n",
    "            features = torch.zeros(len(data_loader.dataset), feats.shape[-1])\n",
    "            if use_cuda:\n",
    "                features = features.cuda(non_blocking=True)\n",
    "            print(f\"Storing features into tensor of shape {features.shape}\")\n",
    "\n",
    "        # get indexes from all processes\n",
    "        y_all = torch.empty(dist.get_world_size(), index.size(0), dtype=index.dtype, device=index.device)\n",
    "        y_l = list(y_all.unbind(0))\n",
    "        y_all_reduce = torch.distributed.all_gather(y_l, index, async_op=True)\n",
    "        y_all_reduce.wait()\n",
    "        index_all = torch.cat(y_l)\n",
    "\n",
    "        # share features between processes\n",
    "        feats_all = torch.empty(\n",
    "            dist.get_world_size(),\n",
    "            feats.size(0),\n",
    "            feats.size(1),\n",
    "            dtype=feats.dtype,\n",
    "            device=feats.device,\n",
    "        )\n",
    "        output_l = list(feats_all.unbind(0))\n",
    "        output_all_reduce = torch.distributed.all_gather(output_l, feats, async_op=True)\n",
    "        output_all_reduce.wait()\n",
    "\n",
    "        # update storage feature matrix\n",
    "        if dist.get_rank() == 0:\n",
    "            if use_cuda:\n",
    "                features.index_copy_(0, index_all, torch.cat(output_l))\n",
    "            else:\n",
    "                features.index_copy_(0, index_all.cpu(), torch.cat(output_l).cpu())\n",
    "    return features\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def knn_classifier(train_features, train_labels, test_features, test_labels, k, T, num_classes=1000):\n",
    "    top1, top5, total = 0.0, 0.0, 0\n",
    "    train_features = train_features.t()\n",
    "    num_test_images, num_chunks = test_labels.shape[0], 100\n",
    "    imgs_per_chunk = num_test_images // num_chunks\n",
    "    retrieval_one_hot = torch.zeros(k, num_classes).to(train_features.device)\n",
    "    for idx in range(0, num_test_images, imgs_per_chunk):\n",
    "        # get the features for test images\n",
    "        features = test_features[\n",
    "            idx : min((idx + imgs_per_chunk), num_test_images), :\n",
    "        ]\n",
    "        targets = test_labels[idx : min((idx + imgs_per_chunk), num_test_images)]\n",
    "        batch_size = targets.shape[0]\n",
    "\n",
    "        # calculate the dot product and compute top-k neighbors\n",
    "        similarity = torch.mm(features, train_features)\n",
    "        distances, indices = similarity.topk(k, largest=True, sorted=True)\n",
    "        candidates = train_labels.view(1, -1).expand(batch_size, -1)\n",
    "        retrieved_neighbors = torch.gather(candidates, 1, indices)\n",
    "\n",
    "        retrieval_one_hot.resize_(batch_size * k, num_classes).zero_()\n",
    "        retrieval_one_hot.scatter_(1, retrieved_neighbors.view(-1, 1), 1)\n",
    "        distances_transform = distances.clone().div_(T).exp_()\n",
    "        probs = torch.sum(\n",
    "            torch.mul(\n",
    "                retrieval_one_hot.view(batch_size, -1, num_classes),\n",
    "                distances_transform.view(batch_size, -1, 1),\n",
    "            ),\n",
    "            1,\n",
    "        )\n",
    "        _, predictions = probs.sort(1, True)\n",
    "\n",
    "        # find the predictions that match the target\n",
    "        correct = predictions.eq(targets.data.view(-1, 1))\n",
    "        top1 = top1 + correct.narrow(1, 0, 1).sum().item()\n",
    "        top5 = top5 + correct.narrow(1, 0, min(5, k)).sum().item()  # top5 does not make sense if k < 5\n",
    "        total += targets.size(0)\n",
    "    top1 = top1 * 100.0 / total\n",
    "    top5 = top5 * 100.0 / total\n",
    "    return top1, top5\n",
    "\n",
    "\n",
    "# class ReturnIndexDataset(datasets.ImageFolder):\n",
    "#     def __getitem__(self, idx):\n",
    "#         img, lab = super(ReturnIndexDataset, self).__getitem__(idx)\n",
    "#         return img, idx\n",
    "\n",
    "class ReturnIndexDataset():\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset=dataset\n",
    "    def __getitem__(self, idx):\n",
    "        img, lab = self.dataset.__getitem__(idx)\n",
    "        return img, idx\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: Evaluation with weighted k-NN on ImageNet [-h]\n",
      "                                                 [--batch_size_per_gpu BATCH_SIZE_PER_GPU]\n",
      "                                                 [--nb_knn NB_KNN [NB_KNN ...]]\n",
      "                                                 [--temperature TEMPERATURE]\n",
      "                                                 [--pretrained_weights PRETRAINED_WEIGHTS]\n",
      "                                                 [--use_cuda USE_CUDA]\n",
      "                                                 [--arch ARCH]\n",
      "                                                 [--patch_size PATCH_SIZE]\n",
      "                                                 [--checkpoint_key CHECKPOINT_KEY]\n",
      "                                                 [--dump_features DUMP_FEATURES]\n",
      "                                                 [--load_features LOAD_FEATURES]\n",
      "                                                 [--num_workers NUM_WORKERS]\n",
      "                                                 [--dist_url DIST_URL]\n",
      "                                                 [--local_rank LOCAL_RANK]\n",
      "Evaluation with weighted k-NN on ImageNet: error: unrecognized arguments: --f=/root/.local/share/jupyter/runtime/kernel-v3ce79120ae9d829dfee231777e52929c447ef2830.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser('Evaluation with weighted k-NN on ImageNet')\n",
    "# parser.add_argument('--batch_size_per_gpu', default=128, type=int, help='Per-GPU batch-size')\n",
    "# parser.add_argument('--nb_knn', default=[10, 20, 100, 200], nargs='+', type=int,\n",
    "#     help='Number of NN to use. 20 is usually working the best.')\n",
    "# parser.add_argument('--temperature', default=0.07, type=float,\n",
    "#     help='Temperature used in the voting coefficient')\n",
    "# parser.add_argument('--pretrained_weights', default='', type=str, help=\"Path to pretrained weights to evaluate.\")\n",
    "# parser.add_argument('--use_cuda', default=True, type=utils.bool_flag,\n",
    "#     help=\"Should we store the features on GPU? We recommend setting this to False if you encounter OOM\")\n",
    "# parser.add_argument('--arch', default='vit_small', type=str, help='Architecture')\n",
    "# parser.add_argument('--patch_size', default=16, type=int, help='Patch resolution of the model.')\n",
    "# parser.add_argument(\"--checkpoint_key\", default=\"teacher\", type=str,\n",
    "#     help='Key to use in the checkpoint (example: \"teacher\")')\n",
    "# parser.add_argument('--dump_features', default=None,\n",
    "#     help='Path where to save computed features, empty for no saving')\n",
    "# parser.add_argument('--load_features', default=None, help=\"\"\"If the features have\n",
    "#     already been computed, where to find them.\"\"\")\n",
    "# parser.add_argument('--num_workers', default=10, type=int, help='Number of data loading workers per GPU.')\n",
    "# parser.add_argument(\"--dist_url\", default=\"env://\", type=str, help=\"\"\"url used to set up\n",
    "#     distributed training; see https://pytorch.org/docs/stable/distributed.html\"\"\")\n",
    "# parser.add_argument(\"--local_rank\", default=0, type=int, help=\"Please ignore and do not set this argument.\")\n",
    "# parser.add_argument('--data_path', default='/path/to/imagenet/', type=str)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "utils.init_distributed_mode(args)\n",
    "print(\"git:\\n  {}\\n\".format(utils.get_sha()))\n",
    "print(\"\\n\".join(\"%s: %s\" % (k, str(v)) for k, v in sorted(dict(vars(args)).items())))\n",
    "cudnn.benchmark = True\n",
    "\n",
    "if args.load_features:\n",
    "    train_features = torch.load(os.path.join(args.load_features, \"trainfeat.pth\"))\n",
    "    test_features = torch.load(os.path.join(args.load_features, \"testfeat.pth\"))\n",
    "    train_labels = torch.load(os.path.join(args.load_features, \"trainlabels.pth\"))\n",
    "    test_labels = torch.load(os.path.join(args.load_features, \"testlabels.pth\"))\n",
    "else:\n",
    "    # need to extract features !\n",
    "    train_features, test_features, train_labels, test_labels = extract_feature_pipeline(args)\n",
    "\n",
    "if utils.get_rank() == 0:\n",
    "    if args.use_cuda:\n",
    "        train_features = train_features.cuda()\n",
    "        test_features = test_features.cuda()\n",
    "        train_labels = train_labels.cuda()\n",
    "        test_labels = test_labels.cuda()\n",
    "\n",
    "    print(\"Features are ready!\\nStart the k-NN classification.\")\n",
    "    for k in args.nb_knn:\n",
    "        top1, top5 = knn_classifier(train_features, train_labels,\n",
    "            test_features, test_labels, k, args.temperature)\n",
    "        print(f\"{k}-NN classifier result: Top1: {top1}, Top5: {top5}\")\n",
    "dist.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_args:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=my_args\n",
    "args.batch_size_per_gpu = 100\n",
    "args.nb_knn = [10, 20, 100, 200]\n",
    "args.temperature = 0.07\n",
    "args.pretrained_weights = '/workspace/puae/dino/outputs/emb10_arcface_m0.3_warmup10/checkpoints/checkpoint.pth'\n",
    "args.use_cuda = True\n",
    "args.arch = \"ResNet head\"\n",
    "args.patch_size = 16\n",
    "args.checkpoint_key = \"teacher\"\n",
    "args.dump_features = None\n",
    "args.load_features = None\n",
    "args.num_workers = 10\n",
    "args.dist_url = \"env://\"\n",
    "args.local_rank = 0\n",
    "args.data_path = \"\"\n",
    "args.emb_dim = 10\n",
    "args.out_filename = \"head_knn_result.txt\"\n",
    "args.port = \"29500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded with 7000 train and 3000 val imgs.\n",
      "Model ResNet emb_dim:10 Built\n",
      "load resnet backbone and head. Then, delete last layer\n",
      "Extracting features for train set...\n",
      "Storing features into tensor of shape torch.Size([7000, 256])\n",
      "  [ 0/70]  eta: 0:00:27    time: 0.395379  data: 0.374108  max mem: 199\n",
      "  [10/70]  eta: 0:00:02    time: 0.040870  data: 0.034190  max mem: 199\n",
      "  [20/70]  eta: 0:00:01    time: 0.004589  data: 0.000156  max mem: 199\n",
      "  [30/70]  eta: 0:00:00    time: 0.003841  data: 0.000133  max mem: 199\n",
      "  [40/70]  eta: 0:00:00    time: 0.003635  data: 0.000143  max mem: 199\n",
      "  [50/70]  eta: 0:00:00    time: 0.004185  data: 0.000141  max mem: 199\n",
      "  [60/70]  eta: 0:00:00    time: 0.004958  data: 0.000131  max mem: 199\n",
      "  [69/70]  eta: 0:00:00    time: 0.004937  data: 0.000086  max mem: 199\n",
      " Total time: 0:00:00 (0.010780 s / it)\n",
      "Extracting features for val set...\n",
      "Storing features into tensor of shape torch.Size([3000, 256])\n",
      "  [ 0/30]  eta: 0:00:10    time: 0.343802  data: 0.322893  max mem: 199\n",
      "  [10/30]  eta: 0:00:00    time: 0.036111  data: 0.029629  max mem: 199\n",
      "  [20/30]  eta: 0:00:00    time: 0.004107  data: 0.000176  max mem: 199\n",
      "  [29/30]  eta: 0:00:00    time: 0.003160  data: 0.000054  max mem: 199\n",
      " Total time: 0:00:00 (0.016791 s / it)\n",
      "Features are ready!\n",
      "Start the k-NN classification.\n",
      "10-NN classifier result: Top1: 89.96666666666667, Top5: 99.03333333333333\n",
      "20-NN classifier result: Top1: 89.56666666666666, Top5: 99.46666666666667\n",
      "100-NN classifier result: Top1: 89.3, Top5: 99.83333333333333\n",
      "200-NN classifier result: Top1: 89.2, Top5: 99.93333333333334\n"
     ]
    }
   ],
   "source": [
    "# utils.init_distributed_mode(args)\n",
    "# print(\"git:\\n  {}\\n\".format(utils.get_sha()))\n",
    "# print(\"\\n\".join(\"%s: %s\" % (k, str(v)) for k, v in sorted(dict(vars(args)).items())))\n",
    "# cudnn.benchmark = True\n",
    "\n",
    "if args.load_features:\n",
    "    train_features = torch.load(os.path.join(args.load_features, \"trainfeat.pth\"))\n",
    "    test_features = torch.load(os.path.join(args.load_features, \"testfeat.pth\"))\n",
    "    train_labels = torch.load(os.path.join(args.load_features, \"trainlabels.pth\"))\n",
    "    test_labels = torch.load(os.path.join(args.load_features, \"testlabels.pth\"))\n",
    "else:\n",
    "    # need to extract features !\n",
    "    train_features, test_features, train_labels, test_labels = extract_feature_pipeline(args)\n",
    "\n",
    "if utils.get_rank() == 0:\n",
    "    if args.use_cuda:\n",
    "        train_features = train_features.cuda()\n",
    "        test_features = test_features.cuda()\n",
    "        train_labels = train_labels.cuda()\n",
    "        test_labels = test_labels.cuda()\n",
    "\n",
    "    results={}\n",
    "    print(\"Features are ready!\\nStart the k-NN classification.\")\n",
    "    for k in args.nb_knn:\n",
    "        top1, top5 = knn_classifier(train_features, train_labels,\n",
    "            test_features, test_labels, k, args.temperature)\n",
    "        print(f\"{k}-NN classifier result: Top1: {top1}, Top5: {top5}\")\n",
    "        results[f\"{k}-NN\"] = {\"Top1\":top1, \"Top5\":top5}\n",
    "dist.barrier()\n",
    "\n",
    "log_file_pth = os.path.dirname(os.path.dirname(args.pretrained_weights))\n",
    "if not os.path.exists(os.path.join(log_file_pth, os.path.join(log_file_pth, args.out_filename))):\n",
    "    with open(os.path.join(log_file_pth, args.out_filename), mode=\"x\") as f:\n",
    "        f.write(f\"{args.pretrained_weights}\\n\")\n",
    "        f.write(f\"{args.arch}\\n\")\n",
    "        for i in results.items():\n",
    "            f.write(str(i[0])+\" \"+str(i[1])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-NN {'Top1': 93.1, 'Top5': 99.6}\n",
      "20-NN {'Top1': 92.06666666666666, 'Top5': 99.73333333333333}\n",
      "100-NN {'Top1': 90.26666666666667, 'Top5': 99.83333333333333}\n",
      "200-NN {'Top1': 89.76666666666667, 'Top5': 99.8}\n"
     ]
    }
   ],
   "source": [
    "for i in results.items():\n",
    "    print(str(i[0]), str(i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_pth = os.path.dirname(os.path.dirname(args.pretrained_weights))\n",
    "with open(os.path.join(log_file_pth, \"knn_result.txt\"), mode=\"x\") as f:\n",
    "    f.write(f\"{args.pretrained_weights}\\n\")\n",
    "    for i in results.items():\n",
    "        f.write(str(i[0])+\" \"+str(i[1])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(os.path.join(log_file_pth, os.path.join(log_file_pth, \"knn_result.txt\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
